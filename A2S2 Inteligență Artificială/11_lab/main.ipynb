{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bfac1b1-970a-406d-bd5b-0fa0f637d740",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c862c41c-517a-4ae9-94c4-3b351ce5391d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 08:53:41.829284: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import typing\n",
    "import json\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import csv\n",
    "import ipywidgets\n",
    "from IPython.display import display\n",
    "import skimage\n",
    "import skimage.feature\n",
    "import math\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eae4462-52f1-4ca4-bba2-db6f1bea191b",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b69b5e-7bc6-4533-8397-1320c86aa16b",
   "metadata": {},
   "source": [
    "## Emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2227f5d8-d3ab-4b07-8e47-537881aa5bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMOJI_PATH = \"./data/emoji/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec19863-da1f-4c05-b578-1376b29d5469",
   "metadata": {},
   "source": [
    "- 0 = Sad\n",
    "- 1 = Happy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3523f7dc-6ded-4d87-bc77-34429ac7e352",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Name to emoticon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03248e18-64c5-4a33-9fa7-cd76693c50c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_emojis = [\n",
    "    \"GRINNING FACE\",\n",
    "    \"GRINNING FACE WITH SMILING EYES\",\n",
    "    \"ROLLING ON THE FLOOR LAUGHING\",\n",
    "    \"FACE WITH TEARS OF JOY\",\n",
    "    \"SLIGHTLY SMILING FACE\",\n",
    "    \"UPSIDE-DOWN FACE\",\n",
    "    \"MELTING FACE\",\n",
    "    \"WINKING FACE\",\n",
    "    \"SMILING FACE WITH SMILING EYES\",\n",
    "    \"SMILING FACE WITH HALO\",\n",
    "    \"KISSING FACE\",\n",
    "    \"KISSING FACE WITH CLOSED EYES\",\n",
    "    \"KISSING FACE WITH SMILING EYES\",\n",
    "    \"SMILING FACE WITH TEAR\",\n",
    "    \"MONEY-MOUTH FACE\",\n",
    "    \"SMIRKING FACE\",\n",
    "    \"RELIEVED FACE\",\n",
    "    \"DROOLING FACE\",\n",
    "    \"SMILING FACE WITH SUNGLASSES\",\n",
    "    \"NERD FACE\",\n",
    "    \"FACE HOLDING BACK TEARS\",\n",
    "    \"SMILING FACE WITH HORNS\",\n",
    "    \"GRINNING FACE WITH SMILING EYES\",\n",
    "    \"CAT FACE WITH TEARS OF JOY\",\n",
    "    \"CAT FACE WITH WRY SMILE\",\n",
    "    \"FACE WITH COWBOY HAT\",\n",
    "    \"FACE THROWING A KISS\",\n",
    "    \"FACE SAVOURING DELICIOUS FOOD\",\n",
    "    \"SMILING FACE WITH SMILING EYES AND HAND COVERING MOUTH\",\n",
    "    \"FACE WITH STUCK-OUT TONGUE\",\n",
    "    \"SMILING CAT FACE WITH OPEN MOUTH\",\n",
    "    \"GRINNING CAT FACE WITH SMILING EYES\",\n",
    "    \"SMILING FACE WITH OPEN MOUTH\",\n",
    "    \"SMILING FACE WITH OPEN MOUTH AND COLD SWEAT\",\n",
    "    \"SMILING FACE WITH OPEN MOUTH AND TIGHTLY-CLOSED EYES\",\n",
    "    \"KISSING CAT FACE WITH CLOSED EYES\",\n",
    "    \"FACE WITH PARTY HORN AND PARTY HAT\",\n",
    "    \"SMILING CAT FACE WITH HEART-SHAPED EYES\",\n",
    "    \"WHITE SMILING FACE\",\n",
    "    \"SMILING FACE WITH HEART-SHAPED EYES\",\n",
    "    \"SMILING FACE WITH SMILING EYES AND THREE HEARTS\",\n",
    "    \"HUGGING FACE\",\n",
    "    \"FACE WITH STUCK-OUT TONGUE AND TIGHTLY-CLOSED EYES\",\n",
    "    \"GRINNING FACE WITH STAR EYES\",\n",
    "    \"FACE WITH STUCK-OUT TONGUE AND WINKING EYE\",\n",
    "    \"GRINNING FACE WITH ONE LARGE AND ONE SMALL EYE\",\n",
    "]\n",
    "\n",
    "sad_emojis = [\n",
    "    \"UNAMUSED FACE\",\n",
    "    \"FACE WITH ROLLING EYES\",\n",
    "    \"GRIMACING FACE\",\n",
    "    \"SHAKING FACE\",\n",
    "    \"FACE EXHALING\",\n",
    "    \"LYING FACE\",\n",
    "    \"PENSIVE FACE\",\n",
    "    \"SLEEPY FACE\",\n",
    "    \"FACE WITH THERMOMETER\",\n",
    "    \"FACE WITH HEAD-BANDAGE\",\n",
    "    \"NAUSEATED FACE\",\n",
    "    \"SNEEZING FACE\",\n",
    "    \"DIZZY FACE\",\n",
    "    \"FACE WITH SPIRAL EYES\",\n",
    "    \"CONFUSED FACE\",\n",
    "    \"WORRIED FACE\",\n",
    "    \"SLIGHTLY FROWNING FACE\",\n",
    "    \"FROWNING FACE\",\n",
    "    \"FACE WITH OPEN MOUTH\",\n",
    "    \"HUSHED FACE\",\n",
    "    \"ASTONISHED FACE\",\n",
    "    \"FROWNING FACE WITH OPEN MOUTH\",\n",
    "    \"ANGUISHED FACE\",\n",
    "    \"FEARFUL FACE\",\n",
    "    \"DISAPPOINTED BUT RELIEVED FACE\",\n",
    "    \"CRYING FACE\",\n",
    "    \"LOUDLY CRYING FACE\",\n",
    "    \"FACE SCREAMING IN FEAR\",\n",
    "    \"CONFOUNDED FACE\",\n",
    "    \"PERSEVERING FACE\",\n",
    "    \"DISAPPOINTED FACE\",\n",
    "    \"WEARY FACE\",\n",
    "    \"TIRED FACE\",\n",
    "    \"ANGRY FACE\",\n",
    "    \"WEARY CAT FACE\",\n",
    "    \"IMP\",\n",
    "    \"FACE WITH OPEN MOUTH AND COLD SWEAT\",\n",
    "    \"CRYING CAT FACE\",\n",
    "    \"FACE WITH COLD SWEAT\",\n",
    "    \"POUTING FACE\",\n",
    "    \"FACE WITH OPEN MOUTH VOMITING\",\n",
    "    \"FACE WITH LOOK OF TRIUMPH\",\n",
    "    \"SERIOUS FACE WITH SYMBOLS COVERING MOUTH\",\n",
    "    \"FACE WITH PLEADING EYES\",\n",
    "    \"POUTING CAT FACE\",\n",
    "]\n",
    "\n",
    "name_to_emotion = dict()\n",
    "\n",
    "for name in happy_emojis:\n",
    "    name_to_emotion[name] = 1\n",
    "for name in sad_emojis:\n",
    "    name_to_emotion[name] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c15987-93dc-44ca-ac2f-8a9d54f62edd",
   "metadata": {},
   "source": [
    "### Rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fab04c25-8757-48c3-ad3c-e641b073a550",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_images = []\n",
    "emoji_emotions = []\n",
    "\n",
    "with open(os.path.join(EMOJI_PATH, \"emoji.json\")) as f:\n",
    "    emoji_json = json.load(f)\n",
    "\n",
    "for entry in emoji_json:\n",
    "    if (emotion := name_to_emotion.get(entry[\"name\"])) is None:\n",
    "        continue\n",
    "    \n",
    "    key_path = [\n",
    "        (\"has_img_apple\", \"img-apple-64\"),\n",
    "        (\"has_img_facebook\", \"img-facebook-64\"),\n",
    "        (\"has_img_google\", \"img-google-64\"),\n",
    "        (\"has_img_twitter\", \"img-twitter-64\"),\n",
    "    ]\n",
    "    for key, idk_path in key_path:\n",
    "        if entry[key]:\n",
    "            path = os.path.join(EMOJI_PATH, idk_path, entry[\"image\"])\n",
    "            with PIL.Image.open(path) as image:\n",
    "                image: PIL.Image = image\n",
    "                image = image.convert(\"RGBA\")\n",
    "                image = image.resize((48, 48))\n",
    "                image_array = np.asarray(image)\n",
    "                \n",
    "                emoji_images.append(image_array)\n",
    "                emoji_emotions.append(emotion)\n",
    "                \n",
    "emoji_images = np.array(emoji_images)\n",
    "emoji_emotions = np.array(emoji_emotions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c49499-3004-4693-bdbf-d24386f9bacc",
   "metadata": {},
   "source": [
    "## Face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa4b70a-5d21-4063-a307-e39c24ca94fa",
   "metadata": {},
   "source": [
    "- 0 = Angry\n",
    "- 1 = Disgust\n",
    "- 2 = Fear\n",
    "- 3 = Happy\n",
    "- 4 = Sad\n",
    "- 5 = Surprise\n",
    "- 6 = Neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9be56e7d-cde1-41eb-a271-ab0ad00bf19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "FACE_PATH = \"./data/face/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e52c0ceb-64eb-4fa0-89dc-c6f4bdeef1d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14a80f057e3c4403bb8a69099c9190db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=35886)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# if False:\n",
    "if True:\n",
    "    image_shape = (48, 48)\n",
    "    \n",
    "    face_emotions = []\n",
    "\n",
    "    with open(os.path.join(FACE_PATH, \"data.csv\"), \"rb\") as f:\n",
    "        row_count = (\n",
    "              sum(1 for _ in f)\n",
    "            - 1 # subtract one to skip header\n",
    "        )\n",
    "\n",
    "    widget_progress = ipywidgets.IntProgress(max=row_count-1)\n",
    "    display(widget_progress)\n",
    "    \n",
    "    face_images = np.zeros((row_count,) + image_shape, dtype=int)\n",
    "    face_emotions = np.zeros((row_count,), dtype=int)\n",
    "    face_extended = np.zeros((row_count, 10), dtype=int)\n",
    "\n",
    "    with (\n",
    "        open(os.path.join(FACE_PATH, \"data.csv\")) as f,\n",
    "        open(os.path.join(FACE_PATH, \"data-extended.csv\")) as g,\n",
    "    ):\n",
    "        f = csv.reader(f)\n",
    "        g = csv.reader(g)\n",
    "        _ = next(f)\n",
    "        _ = next(g)\n",
    "        for i, (row, row_extended) in enumerate(zip(f, g)):\n",
    "            # if i >= 500:\n",
    "            #     break\n",
    "            \n",
    "            face_emotions[i] = int(row[0])\n",
    "            image = np.fromiter((int(val) for val in row[2].split()), int)\n",
    "            image = np.reshape(image, image_shape)\n",
    "            face_images[i] = image\n",
    "            \n",
    "            for j, v in enumerate(row_extended[2:]):\n",
    "                face_extended[i,j] = int(v)\n",
    "\n",
    "            widget_progress.value = i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1621383b-2cbf-4116-b1a5-772727583940",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab55074-0f98-4b17-9c7b-cd539ecc4a28",
   "metadata": {},
   "source": [
    "## Emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59ba45eb-9893-4d5b-8cbc-b09405050dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_21 (Conv2D)          (None, 46, 46, 32)        1184      \n",
      "                                                                 \n",
      " max_pooling2d_14 (MaxPoolin  (None, 23, 23, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_22 (Conv2D)          (None, 21, 21, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_15 (MaxPoolin  (None, 10, 10, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_23 (Conv2D)          (None, 8, 8, 64)          36928     \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 64)                262208    \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 318,946\n",
      "Trainable params: 318,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/32\n",
      "5/5 [==============================] - 1s 101ms/step - loss: 0.7383 - accuracy: 0.4965 - val_loss: 0.6912 - val_accuracy: 0.5278\n",
      "Epoch 2/32\n",
      "5/5 [==============================] - 0s 60ms/step - loss: 0.6949 - accuracy: 0.4755 - val_loss: 0.6843 - val_accuracy: 0.6944\n",
      "Epoch 3/32\n",
      "5/5 [==============================] - 0s 65ms/step - loss: 0.6896 - accuracy: 0.5350 - val_loss: 0.6847 - val_accuracy: 0.5139\n",
      "Epoch 4/32\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 0.6743 - accuracy: 0.6713 - val_loss: 0.6671 - val_accuracy: 0.6389\n",
      "Epoch 5/32\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.6463 - accuracy: 0.6748 - val_loss: 0.6337 - val_accuracy: 0.6389\n",
      "Epoch 6/32\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.5965 - accuracy: 0.6923 - val_loss: 0.6539 - val_accuracy: 0.5694\n",
      "Epoch 7/32\n",
      "5/5 [==============================] - 0s 63ms/step - loss: 0.5680 - accuracy: 0.6853 - val_loss: 0.5648 - val_accuracy: 0.7361\n",
      "Epoch 8/32\n",
      "5/5 [==============================] - 0s 60ms/step - loss: 0.5120 - accuracy: 0.7448 - val_loss: 0.5310 - val_accuracy: 0.6806\n",
      "Epoch 9/32\n",
      "5/5 [==============================] - 0s 61ms/step - loss: 0.4423 - accuracy: 0.7727 - val_loss: 0.4978 - val_accuracy: 0.7361\n",
      "Epoch 10/32\n",
      "5/5 [==============================] - 0s 60ms/step - loss: 0.4234 - accuracy: 0.8007 - val_loss: 0.5186 - val_accuracy: 0.8056\n",
      "Epoch 11/32\n",
      "5/5 [==============================] - 0s 63ms/step - loss: 0.3778 - accuracy: 0.8077 - val_loss: 0.4479 - val_accuracy: 0.7361\n",
      "Epoch 12/32\n",
      "5/5 [==============================] - 0s 58ms/step - loss: 0.3182 - accuracy: 0.8706 - val_loss: 0.4227 - val_accuracy: 0.8194\n",
      "Epoch 13/32\n",
      "5/5 [==============================] - 0s 59ms/step - loss: 0.2639 - accuracy: 0.8986 - val_loss: 0.4586 - val_accuracy: 0.8194\n",
      "Epoch 14/32\n",
      "5/5 [==============================] - 0s 64ms/step - loss: 0.2401 - accuracy: 0.8986 - val_loss: 0.3728 - val_accuracy: 0.8056\n",
      "Epoch 15/32\n",
      "5/5 [==============================] - 0s 57ms/step - loss: 0.1911 - accuracy: 0.9196 - val_loss: 0.3203 - val_accuracy: 0.8472\n",
      "Epoch 16/32\n",
      "5/5 [==============================] - 0s 57ms/step - loss: 0.1399 - accuracy: 0.9685 - val_loss: 0.2461 - val_accuracy: 0.9028\n",
      "Epoch 17/32\n",
      "5/5 [==============================] - 0s 65ms/step - loss: 0.0980 - accuracy: 0.9755 - val_loss: 0.2341 - val_accuracy: 0.9028\n",
      "Epoch 18/32\n",
      "5/5 [==============================] - 0s 58ms/step - loss: 0.0894 - accuracy: 0.9790 - val_loss: 0.2393 - val_accuracy: 0.9028\n",
      "Epoch 19/32\n",
      "5/5 [==============================] - 0s 59ms/step - loss: 0.0695 - accuracy: 0.9790 - val_loss: 0.2137 - val_accuracy: 0.8889\n",
      "Epoch 20/32\n",
      "5/5 [==============================] - 0s 60ms/step - loss: 0.0500 - accuracy: 0.9895 - val_loss: 0.1768 - val_accuracy: 0.9167\n",
      "Epoch 21/32\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.0401 - accuracy: 0.9930 - val_loss: 0.1987 - val_accuracy: 0.9167\n",
      "Epoch 22/32\n",
      "5/5 [==============================] - 0s 63ms/step - loss: 0.0413 - accuracy: 0.9930 - val_loss: 0.2107 - val_accuracy: 0.9583\n",
      "Epoch 23/32\n",
      "5/5 [==============================] - 0s 62ms/step - loss: 0.0306 - accuracy: 0.9965 - val_loss: 0.2083 - val_accuracy: 0.9028\n",
      "Epoch 24/32\n",
      "5/5 [==============================] - 0s 63ms/step - loss: 0.0189 - accuracy: 0.9965 - val_loss: 0.1619 - val_accuracy: 0.9444\n",
      "Epoch 25/32\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 0.0136 - accuracy: 1.0000 - val_loss: 0.1387 - val_accuracy: 0.9444\n",
      "Epoch 26/32\n",
      "5/5 [==============================] - 1s 120ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.1452 - val_accuracy: 0.9444\n",
      "Epoch 27/32\n",
      "5/5 [==============================] - 1s 124ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.1678 - val_accuracy: 0.9306\n",
      "Epoch 28/32\n",
      "5/5 [==============================] - 1s 120ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.1904 - val_accuracy: 0.9306\n",
      "Epoch 29/32\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.1940 - val_accuracy: 0.9167\n",
      "Epoch 30/32\n",
      "3/5 [=================>............] - ETA: 0s - loss: 0.0045 - accuracy: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m), loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     32\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[0;32m---> 34\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m     35\u001b[0m     data_in[split \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m], data_out[split \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     36\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39m(data_in[split \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m], data_out[split \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m]),\n\u001b[1;32m     37\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m     38\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[1;32m     39\u001b[0m )\n",
      "File \u001b[0;32m/nix/store/5kz5gahvnniaz8a7xqv7gdw91ri67r35-python3-3.10.8-env/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/nix/store/5kz5gahvnniaz8a7xqv7gdw91ri67r35-python3-3.10.8-env/lib/python3.10/site-packages/keras/engine/training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1562\u001b[0m ):\n\u001b[1;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/nix/store/5kz5gahvnniaz8a7xqv7gdw91ri67r35-python3-3.10.8-env/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/nix/store/5kz5gahvnniaz8a7xqv7gdw91ri67r35-python3-3.10.8-env/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/nix/store/5kz5gahvnniaz8a7xqv7gdw91ri67r35-python3-3.10.8-env/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/nix/store/5kz5gahvnniaz8a7xqv7gdw91ri67r35-python3-3.10.8-env/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2494\u001b[0m   (graph_function,\n\u001b[1;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nix/store/5kz5gahvnniaz8a7xqv7gdw91ri67r35-python3-3.10.8-env/lib/python3.10/site-packages/tensorflow/python/eager/function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m     args,\n\u001b[1;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1867\u001b[0m     executing_eagerly)\n\u001b[1;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/nix/store/5kz5gahvnniaz8a7xqv7gdw91ri67r35-python3-3.10.8-env/lib/python3.10/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/nix/store/5kz5gahvnniaz8a7xqv7gdw91ri67r35-python3-3.10.8-env/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_percent = 0.80\n",
    "\n",
    "length = len(emoji_images)\n",
    "\n",
    "# preprocess data\n",
    "data_in = emoji_images / 255\n",
    "data_out = np.stack(\n",
    "    [\n",
    "        emoji_emotions == 0,\n",
    "        emoji_emotions == 1,\n",
    "    ],\n",
    "    axis=1\n",
    ").astype(float)\n",
    "\n",
    "split = np.zeros(length, dtype=int)\n",
    "split_index = int(length * test_percent)\n",
    "split[:split_index] = 1\n",
    "\n",
    "np.random.shuffle(split)\n",
    "\n",
    "# train model\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 4)))\n",
    "model.add(keras.layers.MaxPool2D((2, 2)))\n",
    "model.add(keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(keras.layers.MaxPool2D((2, 2)))\n",
    "model.add(keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(64, activation='relu'))\n",
    "model.add(keras.layers.Dense(2, activation='softmax'))\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(\n",
    "    data_in[split == 1], data_out[split == 1],\n",
    "    validation_data=(data_in[split == 0], data_out[split == 0]),\n",
    "    epochs=32,\n",
    "    batch_size=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e335e7-55a6-4107-811d-9b1bacfbdfa7",
   "metadata": {},
   "source": [
    "## Faces (trained, manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240d0673-db36-4c03-a146-d917de13e246",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = face_images[0]\n",
    "image = skimage.transform.rescale(image, 2, anti_aliasing=True)\n",
    "fd, hog_image = skimage.feature.hog(image, orientations=8, pixels_per_cell=(8, 8),\n",
    "                    cells_per_block=(1, 1), visualize=True)\n",
    "\n",
    "# print(hog_image)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), sharex=True, sharey=True)\n",
    "\n",
    "ax1.axis('off')\n",
    "ax1.imshow(image, cmap=plt.cm.gray)\n",
    "ax1.set_title('Input image')\n",
    "\n",
    "# Rescale histogram for better display\n",
    "hog_image_rescaled = skimage.exposure.rescale_intensity(hog_image, in_range=(0, 10))\n",
    "\n",
    "ax2.axis('off')\n",
    "ax2.imshow(hog_image_rescaled, cmap=plt.cm.gray)\n",
    "ax2.set_title('Histogram of Oriented Gradients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9243ad6a-bfaf-4e1e-9e3e-ea321c0d0395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1de326d4286848d8834d3c857e993403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=35886)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if True:\n",
    "    # extract features\n",
    "    data_face_features = np.zeros((len(face_images), 36*32))\n",
    "\n",
    "    widget_progress = ipywidgets.IntProgress(max=len(face_images)-1)\n",
    "    display(widget_progress)\n",
    "\n",
    "    for i, image in enumerate(face_images):\n",
    "        image = skimage.transform.rescale(image, 2, anti_aliasing=True)\n",
    "        data_face_features[i] = skimage.feature.hog(\n",
    "            image,\n",
    "            orientations=8,\n",
    "            pixels_per_cell=(8, 8),\n",
    "            cells_per_block=(1, 1),\n",
    "            feature_vector=True,\n",
    "        )\n",
    "\n",
    "        widget_progress.value = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "641ed662-b1ea-4d0a-a36f-98b8501fd857",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_out = np.stack(\n",
    "    [\n",
    "        face_emotions == 0,\n",
    "        face_emotions == 1,\n",
    "        face_emotions == 2,\n",
    "        face_emotions == 3,\n",
    "        face_emotions == 4,\n",
    "        face_emotions == 5,\n",
    "        face_emotions == 6,\n",
    "    ],\n",
    "    axis=1,\n",
    ").astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97360dd8-de18-4755-bdc2-327d07ad89eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(data_out[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "86cb8924-c61b-4554-9e2b-2ecd65b9bb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_42 (Dense)            (None, 128)               147584    \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 7)                 231       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 174,663\n",
      "Trainable params: 174,663\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/12\n",
      "449/449 [==============================] - 1s 3ms/step - loss: 1.8180 - accuracy: 0.2512 - val_loss: 1.8127 - val_accuracy: 0.2463\n",
      "Epoch 2/12\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 1.8117 - accuracy: 0.2515 - val_loss: 1.8130 - val_accuracy: 0.2463\n",
      "Epoch 3/12\n",
      "449/449 [==============================] - 1s 3ms/step - loss: 1.8112 - accuracy: 0.2515 - val_loss: 1.8122 - val_accuracy: 0.2463\n",
      "Epoch 4/12\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 1.8114 - accuracy: 0.2515 - val_loss: 1.8137 - val_accuracy: 0.2463\n",
      "Epoch 5/12\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 1.8113 - accuracy: 0.2515 - val_loss: 1.8131 - val_accuracy: 0.2463\n",
      "Epoch 6/12\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 1.8114 - accuracy: 0.2515 - val_loss: 1.8129 - val_accuracy: 0.2463\n",
      "Epoch 7/12\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 1.8113 - accuracy: 0.2515 - val_loss: 1.8132 - val_accuracy: 0.2463\n",
      "Epoch 8/12\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 1.8109 - accuracy: 0.2515 - val_loss: 1.8127 - val_accuracy: 0.2463\n",
      "Epoch 9/12\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 1.8112 - accuracy: 0.2515 - val_loss: 1.8136 - val_accuracy: 0.2463\n",
      "Epoch 10/12\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 1.8107 - accuracy: 0.2515 - val_loss: 1.8130 - val_accuracy: 0.2463\n",
      "Epoch 11/12\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 1.8109 - accuracy: 0.2515 - val_loss: 1.8125 - val_accuracy: 0.2463\n",
      "Epoch 12/12\n",
      "449/449 [==============================] - 1s 3ms/step - loss: 1.8107 - accuracy: 0.2515 - val_loss: 1.8124 - val_accuracy: 0.2463\n"
     ]
    }
   ],
   "source": [
    "test_percent = 0.80\n",
    "\n",
    "length = len(face_images)\n",
    "\n",
    "# split into train and test\n",
    "split = np.zeros(length, dtype=int)\n",
    "split_index = int(length * test_percent)\n",
    "split[:split_index] = 1\n",
    "\n",
    "np.random.shuffle(split)\n",
    "\n",
    "# train model\n",
    "model = keras.models.Sequential()\n",
    "# model.add(keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(36, 32, 1)))\n",
    "# model.add(keras.layers.MaxPool2D((2, 2)))\n",
    "# model.add(keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "# model.add(keras.layers.MaxPool2D((2, 2)))\n",
    "# model.add(keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "# model.add(keras.layers.Flatten())\n",
    "\n",
    "model.add(keras.layers.Dense(128, activation='relu', input_shape=(36*32,)))\n",
    "model.add(keras.layers.Dense(128, activation='relu'))\n",
    "model.add(keras.layers.Dense(64, activation='relu'))\n",
    "model.add(keras.layers.Dense(32, activation='relu'))\n",
    "model.add(keras.layers.Dense(7, activation='softmax'))\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# model.build()\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(\n",
    "    data_face_features[split == 1], data_out[split == 1],\n",
    "    validation_data=(data_face_features[split == 0], data_out[split == 0]),\n",
    "    epochs=12,\n",
    "    batch_size=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fce989-d196-475a-933a-4c5760fe26ed",
   "metadata": {},
   "source": [
    "## Faces (trained, automated, multi-label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5c9a2ca4-43b9-4a98-a9d9-41107328db95",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in_faces_multi_label = face_images / 255\n",
    "data_out = (face_extended > 1).astype(float)\n",
    "# data_out = face_extended.astype(float) / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e6fec2e0-eadd-4073-9043-db2f83659f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_27 (Conv2D)          (None, 46, 46, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d_18 (MaxPoolin  (None, 23, 23, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_28 (Conv2D)          (None, 21, 21, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_19 (MaxPoolin  (None, 10, 10, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_29 (Conv2D)          (None, 8, 8, 64)          36928     \n",
      "                                                                 \n",
      " flatten_9 (Flatten)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 64)                262208    \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_54 (Dense)            (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 324,522\n",
      "Trainable params: 324,522\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "449/449 [==============================] - 43s 94ms/step - loss: 0.3582 - accuracy: 0.5293 - val_loss: 0.3167 - val_accuracy: 0.6326\n",
      "Epoch 2/10\n",
      "449/449 [==============================] - 47s 104ms/step - loss: 0.2998 - accuracy: 0.6554 - val_loss: 0.2872 - val_accuracy: 0.6786\n",
      "Epoch 3/10\n",
      "449/449 [==============================] - 45s 99ms/step - loss: 0.2728 - accuracy: 0.6968 - val_loss: 0.2634 - val_accuracy: 0.7134\n",
      "Epoch 4/10\n",
      "449/449 [==============================] - 37s 83ms/step - loss: 0.2539 - accuracy: 0.7203 - val_loss: 0.2521 - val_accuracy: 0.7295\n",
      "Epoch 5/10\n",
      "449/449 [==============================] - 46s 101ms/step - loss: 0.2405 - accuracy: 0.7308 - val_loss: 0.2469 - val_accuracy: 0.7346\n",
      "Epoch 6/10\n",
      "449/449 [==============================] - 44s 97ms/step - loss: 0.2292 - accuracy: 0.7400 - val_loss: 0.2384 - val_accuracy: 0.7212\n",
      "Epoch 7/10\n",
      "449/449 [==============================] - 42s 93ms/step - loss: 0.2192 - accuracy: 0.7462 - val_loss: 0.2400 - val_accuracy: 0.7159\n",
      "Epoch 8/10\n",
      "449/449 [==============================] - 44s 98ms/step - loss: 0.2101 - accuracy: 0.7520 - val_loss: 0.2297 - val_accuracy: 0.7059\n",
      "Epoch 9/10\n",
      "449/449 [==============================] - 50s 111ms/step - loss: 0.2016 - accuracy: 0.7603 - val_loss: 0.2249 - val_accuracy: 0.7412\n",
      "Epoch 10/10\n",
      "449/449 [==============================] - 44s 99ms/step - loss: 0.1929 - accuracy: 0.7671 - val_loss: 0.2251 - val_accuracy: 0.7393\n"
     ]
    }
   ],
   "source": [
    "test_percent = 0.80\n",
    "\n",
    "length = len(face_images)\n",
    "\n",
    "# split into train and test\n",
    "split = np.zeros(length, dtype=int)\n",
    "split_index = int(length * test_percent)\n",
    "split[:split_index] = 1\n",
    "\n",
    "np.random.shuffle(split)\n",
    "\n",
    "# train model\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)))\n",
    "model.add(keras.layers.MaxPool2D((2, 2)))\n",
    "model.add(keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(keras.layers.MaxPool2D((2, 2)))\n",
    "model.add(keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(64, activation='relu'))\n",
    "model.add(keras.layers.Dense(64, activation='relu'))\n",
    "model.add(keras.layers.Dense(32, activation='relu'))\n",
    "model.add(keras.layers.Dense(10, activation='sigmoid'))\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# model.build()\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(\n",
    "    data_in_faces_multi_label[split == 1], data_out[split == 1],\n",
    "    validation_data=(data_in_faces_multi_label[split == 0], data_out[split == 0]),\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    ")\n",
    "\n",
    "model.save(\"model_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9194eef-16b6-43ad-bae6-728f509eeb27",
   "metadata": {},
   "source": [
    "## Faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38de28e8-6a4f-412b-a816-b9b3502a7ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f2b2dab3-477d-478b-a7b2-dc488ca13e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "base_model = MobileNetV2(\n",
    "    input_shape=(48, 48, 3),\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    pooling=\"avg\",\n",
    ")\n",
    "\n",
    "face_images_local = face_images[:length]\n",
    "face_images_rgb = np.stack(\n",
    "    [\n",
    "        face_images_local,\n",
    "        face_images_local,\n",
    "        face_images_local,\n",
    "    ],\n",
    "    axis=3,\n",
    ")\n",
    "\n",
    "face_embedding = base_model(face_images_rgb)\n",
    "face_embedding = keras.layers.Flatten()(face_embedding)\n",
    "face_embedding = keras.layers.Dropout(0.5)(face_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "601081a6-f8c5-465c-8b62-db529b163382",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_emotions_local = face_emotions[:length]\n",
    "data_out = np.stack(\n",
    "    [\n",
    "        face_emotions_local == 0,\n",
    "        face_emotions_local == 1,\n",
    "        face_emotions_local == 2,\n",
    "        face_emotions_local == 3,\n",
    "        face_emotions_local == 4,\n",
    "        face_emotions_local == 5,\n",
    "        face_emotions_local == 6,\n",
    "    ],\n",
    "    axis=1,\n",
    ").astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7bf90748-7b35-4675-920a-da11bbdb5314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_62 (Dense)            (None, 64)                81984     \n",
      "                                                                 \n",
      " dense_63 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_64 (Dense)            (None, 8)                 264       \n",
      "                                                                 \n",
      " dense_65 (Dense)            (None, 8)                 72        \n",
      "                                                                 \n",
      " dense_66 (Dense)            (None, 8)                 72        \n",
      "                                                                 \n",
      " dense_67 (Dense)            (None, 7)                 63        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 84,535\n",
      "Trainable params: 84,535\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/64\n",
      "125/125 [==============================] - 1s 3ms/step - loss: 0.4339 - accuracy: 0.1529 - val_loss: 0.3964 - val_accuracy: 0.2465\n",
      "Epoch 2/64\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.3906 - accuracy: 0.2555 - val_loss: 0.3896 - val_accuracy: 0.2465\n",
      "Epoch 3/64\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.3880 - accuracy: 0.2555 - val_loss: 0.3879 - val_accuracy: 0.2465\n",
      "Epoch 4/64\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.3865 - accuracy: 0.2555 - val_loss: 0.3867 - val_accuracy: 0.2465\n",
      "Epoch 5/64\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.3849 - accuracy: 0.2578 - val_loss: 0.3855 - val_accuracy: 0.2485\n",
      "Epoch 6/64\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.3835 - accuracy: 0.2664 - val_loss: 0.3853 - val_accuracy: 0.2475\n",
      "Epoch 7/64\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.3814 - accuracy: 0.2769 - val_loss: 0.3839 - val_accuracy: 0.2560\n",
      "Epoch 8/64\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.3794 - accuracy: 0.2861 - val_loss: 0.3828 - val_accuracy: 0.2695\n",
      "Epoch 9/64\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.3775 - accuracy: 0.2941 - val_loss: 0.3831 - val_accuracy: 0.2620\n",
      "Epoch 10/64\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.3750 - accuracy: 0.3018 - val_loss: 0.3819 - val_accuracy: 0.2830\n",
      "Epoch 11/64\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.3730 - accuracy: 0.3089 - val_loss: 0.3821 - val_accuracy: 0.2785\n",
      "Epoch 12/64\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.3711 - accuracy: 0.3169 - val_loss: 0.3841 - val_accuracy: 0.2665\n",
      "Epoch 13/64\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.3689 - accuracy: 0.3220 - val_loss: 0.3841 - val_accuracy: 0.2820\n",
      "Epoch 14/64\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.3656 - accuracy: 0.3313 - val_loss: 0.3825 - val_accuracy: 0.2795\n",
      "Epoch 15/64\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.3633 - accuracy: 0.3401 - val_loss: 0.3820 - val_accuracy: 0.2735\n",
      "Epoch 16/64\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 0.3608 - accuracy: 0.3430 - val_loss: 0.3858 - val_accuracy: 0.2795\n",
      "Epoch 17/64\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.3578 - accuracy: 0.3489 - val_loss: 0.3884 - val_accuracy: 0.2780\n",
      "Epoch 18/64\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.3547 - accuracy: 0.3594 - val_loss: 0.3845 - val_accuracy: 0.2765\n",
      "Epoch 19/64\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.3534 - accuracy: 0.3579 - val_loss: 0.3921 - val_accuracy: 0.2785\n",
      "Epoch 20/64\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.3498 - accuracy: 0.3643 - val_loss: 0.3944 - val_accuracy: 0.2795\n",
      "Epoch 21/64\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.3488 - accuracy: 0.3684 - val_loss: 0.3944 - val_accuracy: 0.2875\n",
      "Epoch 22/64\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.3463 - accuracy: 0.3756 - val_loss: 0.3886 - val_accuracy: 0.2780\n",
      "Epoch 23/64\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.3408 - accuracy: 0.3864 - val_loss: 0.4013 - val_accuracy: 0.2650\n",
      "Epoch 24/64\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.3419 - accuracy: 0.3795 - val_loss: 0.3955 - val_accuracy: 0.2800\n",
      "Epoch 25/64\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.3366 - accuracy: 0.3957 - val_loss: 0.4021 - val_accuracy: 0.2715\n",
      "Epoch 26/64\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.3334 - accuracy: 0.3989 - val_loss: 0.4040 - val_accuracy: 0.2705\n",
      "Epoch 27/64\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.3319 - accuracy: 0.4078 - val_loss: 0.4068 - val_accuracy: 0.2755\n",
      "Epoch 28/64\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.3281 - accuracy: 0.4131 - val_loss: 0.4035 - val_accuracy: 0.2645\n",
      "Epoch 29/64\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.3276 - accuracy: 0.4151 - val_loss: 0.4102 - val_accuracy: 0.2690\n",
      "Epoch 30/64\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.3234 - accuracy: 0.4214 - val_loss: 0.4090 - val_accuracy: 0.2775\n",
      "Epoch 31/64\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.3202 - accuracy: 0.4327 - val_loss: 0.4227 - val_accuracy: 0.2860\n",
      "Epoch 32/64\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.3189 - accuracy: 0.4309 - val_loss: 0.4200 - val_accuracy: 0.2610\n",
      "Epoch 33/64\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.3161 - accuracy: 0.4396 - val_loss: 0.4198 - val_accuracy: 0.2700\n",
      "Epoch 34/64\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.3144 - accuracy: 0.4346 - val_loss: 0.4253 - val_accuracy: 0.2705\n",
      "Epoch 35/64\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.3133 - accuracy: 0.4416 - val_loss: 0.4208 - val_accuracy: 0.2675\n",
      "Epoch 36/64\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.3067 - accuracy: 0.4525 - val_loss: 0.4292 - val_accuracy: 0.2680\n",
      "Epoch 37/64\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.3038 - accuracy: 0.4622 - val_loss: 0.4726 - val_accuracy: 0.2815\n",
      "Epoch 38/64\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.3059 - accuracy: 0.4554 - val_loss: 0.4326 - val_accuracy: 0.2580\n",
      "Epoch 39/64\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 0.3056 - accuracy: 0.4616 - val_loss: 0.4404 - val_accuracy: 0.2685\n",
      "Epoch 40/64\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.2954 - accuracy: 0.4761 - val_loss: 0.4437 - val_accuracy: 0.2675\n",
      "Epoch 41/64\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 0.2969 - accuracy: 0.4694 - val_loss: 0.4557 - val_accuracy: 0.2745\n",
      "Epoch 42/64\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.2944 - accuracy: 0.4829 - val_loss: 0.4578 - val_accuracy: 0.2675\n",
      "Epoch 43/64\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 0.2901 - accuracy: 0.4895 - val_loss: 0.4431 - val_accuracy: 0.2555\n",
      "Epoch 44/64\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 0.2891 - accuracy: 0.4889 - val_loss: 0.4672 - val_accuracy: 0.2615\n",
      "Epoch 45/64\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.2892 - accuracy: 0.4880 - val_loss: 0.4914 - val_accuracy: 0.2750\n",
      "Epoch 46/64\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.2843 - accuracy: 0.5009 - val_loss: 0.4812 - val_accuracy: 0.2745\n",
      "Epoch 47/64\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.2803 - accuracy: 0.5136 - val_loss: 0.4772 - val_accuracy: 0.2645\n",
      "Epoch 48/64\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.2785 - accuracy: 0.5114 - val_loss: 0.5078 - val_accuracy: 0.2675\n",
      "Epoch 49/64\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.2768 - accuracy: 0.5265 - val_loss: 0.4840 - val_accuracy: 0.2720\n",
      "Epoch 50/64\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.2772 - accuracy: 0.5170 - val_loss: 0.5040 - val_accuracy: 0.2500\n",
      "Epoch 51/64\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.2730 - accuracy: 0.5299 - val_loss: 0.5034 - val_accuracy: 0.2635\n",
      "Epoch 52/64\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.2735 - accuracy: 0.5295 - val_loss: 0.5043 - val_accuracy: 0.2640\n",
      "Epoch 53/64\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.2677 - accuracy: 0.5374 - val_loss: 0.5087 - val_accuracy: 0.2555\n",
      "Epoch 54/64\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 0.2656 - accuracy: 0.5449 - val_loss: 0.5095 - val_accuracy: 0.2620\n",
      "Epoch 55/64\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.2683 - accuracy: 0.5418 - val_loss: 0.5347 - val_accuracy: 0.2585\n",
      "Epoch 56/64\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.2660 - accuracy: 0.5458 - val_loss: 0.5177 - val_accuracy: 0.2715\n",
      "Epoch 57/64\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.2609 - accuracy: 0.5531 - val_loss: 0.5321 - val_accuracy: 0.2655\n",
      "Epoch 58/64\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.2576 - accuracy: 0.5577 - val_loss: 0.5315 - val_accuracy: 0.2610\n",
      "Epoch 59/64\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.2572 - accuracy: 0.5646 - val_loss: 0.5386 - val_accuracy: 0.2645\n",
      "Epoch 60/64\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.2547 - accuracy: 0.5652 - val_loss: 0.5612 - val_accuracy: 0.2635\n",
      "Epoch 61/64\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.2509 - accuracy: 0.5814 - val_loss: 0.5404 - val_accuracy: 0.2545\n",
      "Epoch 62/64\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.2495 - accuracy: 0.5831 - val_loss: 0.5557 - val_accuracy: 0.2585\n",
      "Epoch 63/64\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.2485 - accuracy: 0.5815 - val_loss: 0.5539 - val_accuracy: 0.2460\n",
      "Epoch 64/64\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.2517 - accuracy: 0.5819 - val_loss: 0.5776 - val_accuracy: 0.2590\n",
      "INFO:tensorflow:Assets written to: model_4/assets\n"
     ]
    }
   ],
   "source": [
    "test_percent = 0.80\n",
    "\n",
    "# split into train and test\n",
    "split = np.zeros(length, dtype=int)\n",
    "split_index = int(length * test_percent)\n",
    "split[:split_index] = 1\n",
    "\n",
    "np.random.shuffle(split)\n",
    "\n",
    "# train model\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Input((1280,)))\n",
    "model.add(keras.layers.Dense(64, activation='relu'))\n",
    "model.add(keras.layers.Dense(32, activation='relu'))\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(7, activation='sigmoid'))\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# model.build()\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(\n",
    "    face_embedding[split == 1], data_out[split == 1],\n",
    "    validation_data=(face_embedding[split == 0], data_out[split == 0]),\n",
    "    epochs=64,\n",
    "    batch_size=64,\n",
    ")\n",
    "\n",
    "model.save(\"model_4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764a5372-755d-4cc6-b250-d098eb2bf6bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
